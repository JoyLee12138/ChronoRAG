import os
import logging
from typing import List, Tuple, Optional
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import AIMessage
from langchain_core.language_models import BaseLanguageModel
from langchain_openai import ChatOpenAI

import httpx
from config import settings
from retriever import chrono_rag_search  
from condense import condense_question   
from prompt import build_chat_rag_prompt

# å¯é€‰ï¼šå¼€å¯è°ƒè¯•æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_rag_llm() -> ChatOpenAI:
    """åˆ›å»ºç”¨äºŽRAGé—®ç­”çš„LLMå®žä¾‹"""
    return ChatOpenAI(
        model="qwen-turbo",
        api_key="",
        base_url=settings.base_url,
        temperature=0.7,
        max_tokens=1024,
        timeout=30,
        max_retries=2,
    )


def query_rag_multiturn(
    question: str,
    chat_history: List[Tuple[str, str]] = None,
    llm: Optional[BaseLanguageModel] = None):
    """
    å¤šè½®å¯¹è¯ RAG ä¸»å…¥å£
    
    Args:
        question (str): ç”¨æˆ·å½“å‰è¾“å…¥çš„é—®é¢˜
        chat_history (List[Tuple[str, str]]): åŽ†å²å¯¹è¯ï¼Œæ ¼å¼ [(ç”¨æˆ·é—®, åŠ©æ‰‹ç­”), ...]
        llm (Optional): å¯ä¼ å…¥è‡ªå®šä¹‰ LLM å®žä¾‹ï¼ˆç”¨äºŽæµ‹è¯•ï¼‰
    
    Returns:
        str: åŠ©æ‰‹çš„å›žç­”
    """
    chat_history = chat_history or []
    
    try:
        # === Step 1: åŽ‹ç¼©é—®é¢˜ï¼ˆå…³é”®ï¼ï¼‰===
        logger.info(f"åŽŸå§‹é—®é¢˜: {question}")
        standalone_question = condense_question(question, chat_history)
        logger.info(f"åŽ‹ç¼©åŽé—®é¢˜: {standalone_question}")

        # === Step 2: RAG æ£€ç´¢ï¼ˆå¤ç”¨ä½ çŽ°æœ‰çš„å·¥å…·ï¼‰===
        # æ³¨æ„ï¼šchrono_rag_search æ˜¯ LangChain Toolï¼Œè¾“å…¥æ˜¯ dict
        context = chrono_rag_search.invoke({"query": standalone_question})
        logger.info(f"æ£€ç´¢åˆ°ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")

        # === Step 3: æž„å»º Prompt ===
        # ä»…ä¿ç•™æœ€è¿‘ 1ï½ž2 è½®å¯¹è¯ï¼Œé¿å…è¶…ä¸Šä¸‹æ–‡
        recent_history = ""
        if chat_history:
            # å–æœ€è¿‘ 1 è½®ï¼ˆå¹³è¡¡ç›¸å…³æ€§ä¸Žé•¿åº¦ï¼‰
            last_q, last_a = chat_history[-1]
            recent_history = f"ç”¨æˆ·ï¼š{last_q}\nåŠ©æ‰‹ï¼š{last_a}"

        prompt = build_chat_rag_prompt(
            question=question,
            context=context,
            chat_history=recent_history
        )
        logger.debug(f"æœ€ç»ˆ Prompt:\n{prompt}")

        # === Step 4: è°ƒç”¨ä¸» LLM ç”Ÿæˆå›žç­” ===
        if llm is None:
            llm = get_rag_llm()
        
        response = llm.invoke(prompt)
        answer = response.content.strip() if hasattr(response, 'content') else str(response).strip()

        # === Step 5: å®‰å…¨åŽå¤„ç† ===
        if not answer:
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›žç­”ã€‚"
        
        return answer

    except Exception as e:
        logger.error(f"query_rag_multiturn å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return f"âŒ ç³»ç»Ÿå¼‚å¸¸ï¼š{str(e)}"


# ===== æœ¬åœ°è°ƒè¯•å…¥å£ =====
if __name__ == "__main__":
    

    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    history = []

    print("ðŸ¤– æ¬¢è¿Žä½¿ç”¨ ChronoRAG-ZH åŽ†å²é—®ç­”åŠ©æ‰‹ï¼è¾“å…¥ 'é€€å‡º' ç»“æŸã€‚\n")
    while True:
        user_input = input("ðŸ‘¤ ç”¨æˆ·: ").strip()
        if not user_input or user_input.lower() in ["é€€å‡º", "quit", "exit"]:
            break

        answer = query_rag_multiturn(user_input, history)
        print(f"\nðŸ¤– åŠ©æ‰‹: {answer}\n")

        # ä¿å­˜åŽŸå§‹é—®ç­”å¯¹ï¼ˆç”¨äºŽä¸‹ä¸€è½® condenseï¼‰
        history.append((user_input, answer))

        # å¯é€‰ï¼šé™åˆ¶åŽ†å²é•¿åº¦ï¼ˆé˜²æ­¢è¿‡é•¿ï¼‰
        if len(history) > 3:
            history = history[-3:]